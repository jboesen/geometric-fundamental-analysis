{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Y.json', 'r') as file:\n",
    "    y = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = {}\n",
    "all_cols = None\n",
    "y_list = []\n",
    "for file in os.listdir('filtered_dt_428'):\n",
    "    if file.endswith('.csv'):\n",
    "        k = file.split('_')[0]\n",
    "        if k in y:\n",
    "            dt[k] = pd.read_csv(os.path.join('filtered_dt_428', file))\n",
    "            dt[k] = dt[k].loc[:, ~dt[k].columns.duplicated()]\n",
    "            dt[k] = dt[k].reindex(sorted(dt[k].columns), axis=1)\n",
    "            y_list.append(y[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_columns(df):\n",
    "    \"\"\"\n",
    "    Function to parse column names and remove the {ticker}: prefix.\n",
    "    \"\"\"\n",
    "    new_columns = []\n",
    "    for col in df.columns:\n",
    "        if \":\" in col:\n",
    "            ticker, real_col_name = col.split(\":\")\n",
    "            new_columns.append(real_col_name)\n",
    "        else:\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "# Get the union of all column names across the DataFrames\n",
    "all_cols = set().union(*[set(parse_columns(df)) for df in dt.values()])\n",
    "list(all_cols)\n",
    "\n",
    "# Update the column names for each DataFrame\n",
    "for k in list(dt.keys()):\n",
    "    dt[k].columns = parse_columns(dt[k])\n",
    "    try:\n",
    "        dt[k] = dt[k].reindex(columns=all_cols, fill_value=0)\n",
    "    except Exception as e:\n",
    "        print(k, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes\n",
    "padded_tensors = []\n",
    "max_length = 7\n",
    "for df in dt.values():\n",
    "    num_cols = df.columns[df.dtypes != 'object']\n",
    "    df = df[num_cols]\n",
    "    # get first 7 rows\n",
    "    padded_tensor = torch.tensor(df.iloc[-7:].values.tolist())\n",
    "    padded_tensors.append(padded_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T x batch size x features\n",
    "X = torch.stack(padded_tensors, dim=1)\n",
    "\n",
    "mean_tensor = torch.mean(X, dim=(0, 1), keepdim=True)\n",
    "std_tensor = torch.std(X, dim=(0, 1), unbiased=False, keepdim=True)\n",
    "X = (X - mean_tensor) / std_tensor\n",
    "X = X.permute(1, 0, 2)\n",
    "y = y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[2]  # Number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "dataset = TensorDataset(X, torch.tensor(y))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_size).requires_grad_()\n",
    "\n",
    "        _, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(hn.view(-1, self.hidden_size))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[2]  # Number of features\n",
    "output_size = 1  # Single output for price prediction\n",
    "hidden_size = 64  # Number of hidden units in the LSTM\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    val_loss /= len(val_dataset) * 10 \n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2102630215946028"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        if loss.item() < 50 or torch.any(outputs > 50):\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "val_loss /= len(val_dataset)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([x.squeeze(1) for x in preds], dim=0).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "pca = PCA().fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evr = pca.explained_variance_ratio_\n",
    "# Calculate the cumulative sum of the explained variance ratio\n",
    "cumulative_var_ratio = np.cumsum(evr)\n",
    "\n",
    "# Plot the cumulative sum of the explained variance ratio\n",
    "plt.plot(range(1, len(cumulative_var_ratio) + 1), cumulative_var_ratio)\n",
    "plt.xlabel('Number of dimensions')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.title('Explained variance ratio vs. Number of dimensions')\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number of dimensions\n",
    "optimal_dim = np.argmax(cumulative_var_ratio >= 0.80) + 1\n",
    "print('Optimal number of dimensions:', optimal_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=optimal_dim)\n",
    "pca = pca.fit(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_compressed = torch.zeros(X.shape[0], X.shape[1], optimal_dim)\n",
    "for k in range(X.shape[0]):\n",
    "    X_compressed[k, :, :] = torch.tensor(pca.transform(X[k, :, :]))\n",
    "\n",
    "X_compressed_val = torch.zeros(X_val.shape[0], X_val.shape[1], optimal_dim)\n",
    "for k in range(X_val.shape[0]):\n",
    "    X_compressed_val[k, :, :] = torch.tensor(pca.transform(X[k, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "dataset = TensorDataset(X_compressed, torch.tensor(y))\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_compressed.shape[2]  # Number of features\n",
    "output_size = 1  # Single output for price prediction\n",
    "hidden_size = 64  # Number of hidden units in the LSTM\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    val_loss /= len(val_dataset)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([466, 7, 15])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_compressed_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        preds.append(outputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        if loss.item() < 50 or torch.any(outputs > 50):\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "val_loss /= len(val_dataset)\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([x.squeeze(1) for x in preds], dim=0).std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "220-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
