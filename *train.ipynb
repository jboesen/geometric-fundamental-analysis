{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnboesen/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/johnboesen/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from diffpool_helpers.model.diffpool_continuous import TSDiffPool\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import curvlearn as cv\n",
    "from curvlearn.manifolds.manifold import Manifold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import time\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'rb') as file:\n",
    "    test_set = pickle.load(file)\n",
    "\n",
    "with open('train_set.pkl', 'rb') as file:\n",
    "    train_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph, _ in train_set:\n",
    "    for key, value in graph.ndata.items():\n",
    "        graph.ndata[key] = value.float()\n",
    "for graph, _ in test_set:\n",
    "    for key, value in graph.ndata.items():\n",
    "        graph.ndata[key] = value.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_batched = []\n",
    "test_batched = []\n",
    "for i in range(len(train_set) // batch_size):\n",
    "    idx_low = batch_size * i\n",
    "    idx_high = (i + 1) * batch_size\n",
    "    y_i = [x[1] for x in train_set[idx_low:idx_high]]\n",
    "    x_i = [x[0] for x in train_set[idx_low:idx_high]]\n",
    "    if not x_i or not y_i: continue\n",
    "    train_batched.append((dgl.batch(x_i), y_i))\n",
    "\n",
    "for i in range(len(test_set) // batch_size):\n",
    "    idx_low = batch_size * i\n",
    "    idx_high = (batch_size + 1) * i\n",
    "    y_i = [x[1] for x in test_set[idx_low:idx_high]]\n",
    "    x_i = [x[0] for x in test_set[idx_low:idx_high]]\n",
    "    if not x_i or not y_i: continue\n",
    "    test_batched.append((dgl.batch(x_i), y_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args():\n",
    "    args = argparse.Namespace(\n",
    "        dataset='default_dataset',\n",
    "        save_dir='checkpoints',\n",
    "        epoch=150,\n",
    "        cuda=0,\n",
    "        clip=0.5,\n",
    "        # input_dim=None,\n",
    "        # hidden_dim=64,\n",
    "        input_dim=2,\n",
    "        hidden_dim=2,\n",
    "        embedding_dim=16,\n",
    "        activation='relu',\n",
    "        n_layers=3,\n",
    "        dropout=0.5,\n",
    "        n_pooling=1,\n",
    "        linkpred=False,\n",
    "        batch_size=1,\n",
    "        aggregator_type='mean',\n",
    "        assign_dim=1,\n",
    "        pool_ratio=0.5,\n",
    "        cat=True\n",
    "    )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 4\n",
    "embedding_dim = 4\n",
    "model = TSDiffPool(\n",
    "    # input_dim,\n",
    "    # hidden_dim,\n",
    "    input_dim=2,\n",
    "    hidden_dim=1,\n",
    "    embedding_dim=1, # idk why this would be 21...\n",
    "    label_dim = batch_size, # linear task\n",
    "    activation=nn.ReLU(),\n",
    "    n_layers = 4,\n",
    "    dropout=0.5,\n",
    "    n_pooling=1,\n",
    "    linkpred=False,\n",
    "    batch_size=1,\n",
    "    aggregator_type=\"meanpool\",\n",
    "    assign_dim=8,\n",
    "    pool_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, prog_args, same_feat=True, val_dataset=None):\n",
    "    \"\"\"\n",
    "    training function\n",
    "    \"\"\"\n",
    "    dir = prog_args.save_dir + \"/\" + prog_args.dataset\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    dataloader = dataset\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.005\n",
    "    )\n",
    "    early_stopping_logger = {\"best_epoch\": -1, \"val_acc\": -1}\n",
    "\n",
    "    if prog_args.cuda > 0:\n",
    "        torch.cuda.set_device(0)\n",
    "    losses = []\n",
    "    # max int\n",
    "    best_test = float('inf')\n",
    "    for epoch in range(prog_args.epoch):\n",
    "        begin_time = time.time()\n",
    "        model.train()\n",
    "        accum_correct = 0\n",
    "        total = 0\n",
    "        epoch_loss = []\n",
    "        # print(\"\\nEPOCH ###### {} ######\".format(epoch))\n",
    "        y_preds = torch.zeros((len(dataloader)))\n",
    "        computation_time = 0.0\n",
    "        for batch_idx, (batch_graph, graph_labels) in enumerate(dataloader):\n",
    "            for key, value in batch_graph.ndata.items():\n",
    "                batch_graph.ndata[key] = value.float()\n",
    "            # graph_labels = graph_labels.long()\n",
    "            if torch.cuda.is_available():\n",
    "                batch_graph = batch_graph.to(torch.cuda.current_device())\n",
    "                graph_labels = graph_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            compute_start = time.time()\n",
    "            ypred = model(batch_graph)\n",
    "            y_preds[batch_idx] = torch.mean(ypred).item()\n",
    "            loss = model.loss(ypred, torch.tensor([graph_labels], dtype=torch.float32))\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            batch_compute_time = time.time() - compute_start\n",
    "            computation_time += batch_compute_time\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), prog_args.clip)\n",
    "            optimizer.step()\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(f\"EPOCH {epoch} LOSS {np.mean(epoch_loss)}\")\n",
    "        print(f\"Predictions: {y_preds.mean()}\")\n",
    "\n",
    "        # train_accu = accum_correct / total\n",
    "        # print(\n",
    "        #     \"train accuracy for this epoch {} is {:.2f}%\".format(\n",
    "        #         epoch, train_accu * 100\n",
    "        #     )\n",
    "        # )\n",
    "        elapsed_time = time.time() - begin_time\n",
    "        # print(\n",
    "        #     \"loss {:.4f} with epoch time {:.4f} s & computation time {:.4f} s \".format(\n",
    "        #         loss.item(), elapsed_time, computation_time\n",
    "        #     )\n",
    "        # )\n",
    "        test_losses = []\n",
    "        for batch_graph, graph_labels in test_set:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_graph = batch_graph.to(torch.cuda.current_device())\n",
    "                graph_labels = graph_labels.cuda()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                ypred = model(batch_graph)\n",
    "                loss = model.loss(ypred, torch.tensor([graph_labels], dtype=torch.float32))\n",
    "                test_losses.append(loss.item())\n",
    "        mean_test_loss = np.mean(test_losses)\n",
    "        if mean_test_loss < best_test:\n",
    "            best_val = mean_test_loss\n",
    "            early_stopping_logger[\"best_epoch\"] = epoch\n",
    "            early_stopping_logger[\"val_acc\"] = mean_test_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                dir + \"/model_{}_{}.pth\".format(prog_args.dataset, epoch),\n",
    "            )\n",
    "        else:\n",
    "            print(\"Test loss is greater than the best val loss. Quitting...\")\n",
    "            return losses\n",
    "        torch.cuda.empty_cache()\n",
    "    return losses, early_stopping_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnboesen/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1, 20])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 LOSS 1.1753372140228748\n",
      "Predictions: 0.18547384440898895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnboesen/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 LOSS 0.6823450781834813\n",
      "Predictions: 0.5592571496963501\n",
      "EPOCH 2 LOSS 0.48327734604334605\n",
      "Predictions: 0.8321808576583862\n",
      "EPOCH 3 LOSS 0.4451091952012995\n",
      "Predictions: 0.9521535038948059\n",
      "EPOCH 4 LOSS 0.43909420716003156\n",
      "Predictions: 0.9929077625274658\n",
      "EPOCH 5 LOSS 0.43803663345841837\n",
      "Predictions: 1.0046790838241577\n",
      "EPOCH 6 LOSS 0.4377803383395076\n",
      "Predictions: 1.008047342300415\n",
      "EPOCH 7 LOSS 0.43770064261312097\n",
      "Predictions: 1.0091060400009155\n",
      "EPOCH 8 LOSS 0.4376671500862218\n",
      "Predictions: 1.0094534158706665\n",
      "EPOCH 9 LOSS 0.437654598042942\n",
      "Predictions: 1.0096046924591064\n",
      "EPOCH 10 LOSS 0.4376512602186547\n",
      "Predictions: 1.0096796751022339\n",
      "EPOCH 11 LOSS 0.4376523168578457\n",
      "Predictions: 1.0097160339355469\n",
      "EPOCH 12 LOSS 0.4376555438726567\n",
      "Predictions: 1.0097345113754272\n",
      "EPOCH 13 LOSS 0.43765962734603536\n",
      "Predictions: 1.0097451210021973\n",
      "EPOCH 14 LOSS 0.4376640225080057\n",
      "Predictions: 1.009752631187439\n",
      "EPOCH 15 LOSS 0.43766838937209773\n",
      "Predictions: 1.0097589492797852\n",
      "EPOCH 16 LOSS 0.43767251769224036\n",
      "Predictions: 1.0097647905349731\n",
      "EPOCH 17 LOSS 0.43767636880063665\n",
      "Predictions: 1.0097702741622925\n",
      "EPOCH 18 LOSS 0.43767993436911357\n",
      "Predictions: 1.0097754001617432\n",
      "EPOCH 19 LOSS 0.43768315207069886\n",
      "Predictions: 1.0097804069519043\n",
      "EPOCH 20 LOSS 0.43768602337401646\n",
      "Predictions: 1.0097849369049072\n",
      "EPOCH 21 LOSS 0.4376886420381757\n",
      "Predictions: 1.0097891092300415\n",
      "EPOCH 22 LOSS 0.43769093982588786\n",
      "Predictions: 1.0097929239273071\n",
      "EPOCH 23 LOSS 0.43769307633360416\n",
      "Predictions: 1.009796380996704\n",
      "EPOCH 24 LOSS 0.43769490516457993\n",
      "Predictions: 1.0097997188568115\n",
      "EPOCH 25 LOSS 0.43769657615428936\n",
      "Predictions: 1.0098024606704712\n",
      "EPOCH 26 LOSS 0.43769806881363577\n",
      "Predictions: 1.0098049640655518\n",
      "EPOCH 27 LOSS 0.4376994226880085\n",
      "Predictions: 1.0098073482513428\n",
      "EPOCH 28 LOSS 0.4377005639521835\n",
      "Predictions: 1.0098094940185547\n",
      "EPOCH 29 LOSS 0.43770168519292313\n",
      "Predictions: 1.009811282157898\n",
      "EPOCH 30 LOSS 0.4377025667972003\n",
      "Predictions: 1.0098130702972412\n",
      "EPOCH 31 LOSS 0.43770345305809033\n",
      "Predictions: 1.0098143815994263\n",
      "EPOCH 32 LOSS 0.4377041811732432\n",
      "Predictions: 1.0098159313201904\n",
      "EPOCH 33 LOSS 0.4377048941006741\n",
      "Predictions: 1.009817123413086\n",
      "EPOCH 34 LOSS 0.43770550386621976\n",
      "Predictions: 1.0098183155059814\n",
      "EPOCH 35 LOSS 0.4377060394841604\n",
      "Predictions: 1.0098192691802979\n",
      "EPOCH 36 LOSS 0.4377065578726335\n",
      "Predictions: 1.0098202228546143\n",
      "EPOCH 37 LOSS 0.4377069888421549\n",
      "Predictions: 1.0098209381103516\n",
      "EPOCH 38 LOSS 0.4377074014717856\n",
      "Predictions: 1.0098217725753784\n",
      "EPOCH 39 LOSS 0.4377077682695996\n",
      "Predictions: 1.0098224878311157\n",
      "EPOCH 40 LOSS 0.4377081072351967\n",
      "Predictions: 1.009822964668274\n",
      "EPOCH 41 LOSS 0.4377083554684829\n",
      "Predictions: 1.0098235607147217\n",
      "EPOCH 42 LOSS 0.43770862822062695\n",
      "Predictions: 1.0098240375518799\n",
      "EPOCH 43 LOSS 0.4377088975161314\n",
      "Predictions: 1.009824514389038\n",
      "EPOCH 44 LOSS 0.4377090569513922\n",
      "Predictions: 1.0098248720169067\n",
      "EPOCH 45 LOSS 0.4377092164582931\n",
      "Predictions: 1.0098252296447754\n",
      "EPOCH 46 LOSS 0.43770946856015003\n",
      "Predictions: 1.0098254680633545\n",
      "EPOCH 47 LOSS 0.43770955082100743\n",
      "Predictions: 1.0098258256912231\n",
      "EPOCH 48 LOSS 0.43770969469243515\n",
      "Predictions: 1.0098261833190918\n",
      "EPOCH 49 LOSS 0.43770982957301807\n",
      "Predictions: 1.009826421737671\n",
      "EPOCH 50 LOSS 0.43770992073517007\n",
      "Predictions: 1.0098265409469604\n",
      "EPOCH 51 LOSS 0.43771001685840577\n",
      "Predictions: 1.0098267793655396\n",
      "EPOCH 52 LOSS 0.4377101703833502\n",
      "Predictions: 1.0098270177841187\n",
      "EPOCH 53 LOSS 0.4377102019229474\n",
      "Predictions: 1.0098271369934082\n",
      "EPOCH 54 LOSS 0.4377102769839649\n",
      "Predictions: 1.0098272562026978\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses, early_stopping_logger \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_default_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msame_feat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, model, prog_args, same_feat, val_dataset)\u001b[0m\n\u001b[1;32m     36\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     37\u001b[0m compute_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 38\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m y_preds[batch_idx] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(ypred)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(ypred, torch\u001b[38;5;241m.\u001b[39mtensor([graph_labels], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/diffpool_helpers/model/diffpool_continuous.py:192\u001b[0m, in \u001b[0;36mTSDiffPool.forward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m    189\u001b[0m out_all \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# we use GCN blocks to get an embedding first\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m g_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcn_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgc_before_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m g\u001b[38;5;241m.\u001b[39mndata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m g_embedding\n\u001b[1;32m    196\u001b[0m readout \u001b[38;5;241m=\u001b[39m dgl\u001b[38;5;241m.\u001b[39msum_nodes(g, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/diffpool_helpers/model/diffpool_continuous.py:162\u001b[0m, in \u001b[0;36mTSDiffPool.gcn_forward\u001b[0;34m(self, g, h, gc_layers, cat)\u001b[0m\n\u001b[1;32m    160\u001b[0m     h \u001b[38;5;241m=\u001b[39m gc_layer(g, h)\n\u001b[1;32m    161\u001b[0m     block_readout\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[0;32m--> 162\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mgc_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m block_readout\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cat:\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/diffpool_helpers/model/dgl_layers/gnn.py:53\u001b[0m, in \u001b[0;36mGraphSageLayer.forward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m     51\u001b[0m     device \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(h\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 53\u001b[0m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_u\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbundler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bn:\n\u001b[1;32m     55\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(h)\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/dgl/heterograph.py:5112\u001b[0m, in \u001b[0;36mDGLGraph.update_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func, etype)\u001b[0m\n\u001b[1;32m   5110\u001b[0m _, dtid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mmetagraph\u001b[38;5;241m.\u001b[39mfind_edge(etid)\n\u001b[1;32m   5111\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m etype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m[etype]\n\u001b[0;32m-> 5112\u001b[0m ndata \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage_passing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_node_func\u001b[49m\n\u001b[1;32m   5114\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5116\u001b[0m     core\u001b[38;5;241m.\u001b[39mis_builtin(reduce_func)\n\u001b[1;32m   5117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m reduce_func\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   5118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ndata\n\u001b[1;32m   5119\u001b[0m ):\n\u001b[1;32m   5120\u001b[0m     \u001b[38;5;66;03m# Replace infinity with zero for isolated nodes\u001b[39;00m\n\u001b[1;32m   5121\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ndata\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/dgl/core.py:415\u001b[0m, in \u001b[0;36mmessage_passing\u001b[0;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m         orig_nid \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mdstdata\u001b[38;5;241m.\u001b[39mget(NID, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 415\u001b[0m         ndata \u001b[38;5;241m=\u001b[39m \u001b[43minvoke_udf_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsgdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_nid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_nid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# apply phase\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m afunc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/dgl/core.py:146\u001b[0m, in \u001b[0;36minvoke_udf_reduce\u001b[0;34m(graph, func, msgdata, orig_nid)\u001b[0m\n\u001b[1;32m    144\u001b[0m eid_bkt \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mzerocopy_to_numpy(graph\u001b[38;5;241m.\u001b[39min_edges(node_bkt, form\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meid\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eid_bkt) \u001b[38;5;241m==\u001b[39m deg \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(node_bkt)\n\u001b[0;32m--> 146\u001b[0m eid_bkt \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43meid_bkt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_bkt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m eid_bkt \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mzerocopy_from_numpy(eid_bkt\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[1;32m    149\u001b[0m msgdata_bkt \u001b[38;5;241m=\u001b[39m msgdata\u001b[38;5;241m.\u001b[39msubframe(eid_bkt)\n",
      "File \u001b[0;32m~/Documents/Code/#Harvard/220-proj/220-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1016\u001b[0m, in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m a\u001b[38;5;241m.\u001b[39msort(axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, early_stopping_logger = train(train_batched, model, get_default_args(), same_feat=True, val_dataset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "file_path = 'model_52_4_PL.pkl'\n",
    "\n",
    "# Save the model object as a pickle file\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "220-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
