{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffpool_helpers.model.diffpool_continuous import TSDiffPool\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import curvlearn as cv\n",
    "from curvlearn.manifolds.manifold import Manifold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import time\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'rb') as file:\n",
    "    test_set = pickle.load(file)\n",
    "\n",
    "with open('train_set.pkl', 'rb') as file:\n",
    "    train_set = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for graph, _ in train_set:\n",
    "    for key, value in graph.ndata.items():\n",
    "        graph.ndata[key] = value.float()\n",
    "for graph, _ in test_set:\n",
    "    for key, value in graph.ndata.items():\n",
    "        graph.ndata[key] = value.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_batched = []\n",
    "test_batched = []\n",
    "for i in range(len(train_set) // batch_size):\n",
    "    idx_low = batch_size * i\n",
    "    idx_high = (i + 1) * batch_size\n",
    "    y_i = [x[1] for x in train_set[idx_low:idx_high]]\n",
    "    x_i = [x[0] for x in train_set[idx_low:idx_high]]\n",
    "    if not x_i or not y_i: continue\n",
    "    train_batched.append((dgl.batch(x_i), y_i))\n",
    "\n",
    "for i in range(len(test_set) // batch_size):\n",
    "    idx_low = batch_size * i\n",
    "    idx_high = (batch_size + 1) * i\n",
    "    y_i = [x[1] for x in test_set[idx_low:idx_high]]\n",
    "    x_i = [x[0] for x in test_set[idx_low:idx_high]]\n",
    "    if not x_i or not y_i: continue\n",
    "    test_batched.append((dgl.batch(x_i), y_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_args():\n",
    "    args = argparse.Namespace(\n",
    "        dataset='default_dataset',\n",
    "        save_dir='checkpoints',\n",
    "        epoch=150,\n",
    "        cuda=0,\n",
    "        clip=0.5,\n",
    "        # input_dim=None,\n",
    "        # hidden_dim=64,\n",
    "        input_dim=2,\n",
    "        hidden_dim=2,\n",
    "        embedding_dim=16,\n",
    "        activation='relu',\n",
    "        n_layers=3,\n",
    "        dropout=0.5,\n",
    "        n_pooling=1,\n",
    "        linkpred=False,\n",
    "        batch_size=1,\n",
    "        aggregator_type='mean',\n",
    "        assign_dim=1,\n",
    "        pool_ratio=0.5,\n",
    "        cat=True\n",
    "    )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "hidden_dim = 4\n",
    "embedding_dim = 4\n",
    "model = TSDiffPool(\n",
    "    # input_dim,\n",
    "    # hidden_dim,\n",
    "    input_dim=2,\n",
    "    hidden_dim=1,\n",
    "    embedding_dim=1, # idk why this would be 21...\n",
    "    label_dim = batch_size, # linear task\n",
    "    activation=nn.ReLU(),\n",
    "    n_layers = 4,\n",
    "    dropout=0.5,\n",
    "    n_pooling=1,\n",
    "    linkpred=False,\n",
    "    batch_size=1,\n",
    "    aggregator_type=\"meanpool\",\n",
    "    assign_dim=8,\n",
    "    pool_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, prog_args, same_feat=True, val_dataset=None):\n",
    "    \"\"\"\n",
    "    training function\n",
    "    \"\"\"\n",
    "    dir = prog_args.save_dir + \"/\" + prog_args.dataset\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    dataloader = dataset\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.005\n",
    "    )\n",
    "    early_stopping_logger = {\"best_epoch\": -1, \"val_acc\": -1}\n",
    "\n",
    "    if prog_args.cuda > 0:\n",
    "        torch.cuda.set_device(0)\n",
    "    losses = []\n",
    "    # max int\n",
    "    best_test = float('inf')\n",
    "    for epoch in range(prog_args.epoch):\n",
    "        begin_time = time.time()\n",
    "        model.train()\n",
    "        accum_correct = 0\n",
    "        total = 0\n",
    "        epoch_loss = []\n",
    "        # print(\"\\nEPOCH ###### {} ######\".format(epoch))\n",
    "        y_preds = torch.zeros((len(dataloader)))\n",
    "        computation_time = 0.0\n",
    "        for batch_idx, (batch_graph, graph_labels) in enumerate(dataloader):\n",
    "            for key, value in batch_graph.ndata.items():\n",
    "                batch_graph.ndata[key] = value.float()\n",
    "            # graph_labels = graph_labels.long()\n",
    "            if torch.cuda.is_available():\n",
    "                batch_graph = batch_graph.to(torch.cuda.current_device())\n",
    "                graph_labels = graph_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            compute_start = time.time()\n",
    "            ypred = model(batch_graph)\n",
    "            y_preds[batch_idx] = torch.mean(ypred).item()\n",
    "            loss = model.loss(ypred, torch.tensor([graph_labels], dtype=torch.float32))\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            batch_compute_time = time.time() - compute_start\n",
    "            computation_time += batch_compute_time\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), prog_args.clip)\n",
    "            optimizer.step()\n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(f\"EPOCH {epoch} LOSS {np.mean(epoch_loss)}\")\n",
    "        print(f\"Predictions: {y_preds.mean()}\")\n",
    "\n",
    "        elapsed_time = time.time() - begin_time\n",
    "        test_losses = []\n",
    "        for batch_graph, graph_labels in test_set:\n",
    "            if torch.cuda.is_available():\n",
    "                batch_graph = batch_graph.to(torch.cuda.current_device())\n",
    "                graph_labels = graph_labels.cuda()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                ypred = model(batch_graph)\n",
    "                loss = model.loss(ypred, torch.tensor([graph_labels], dtype=torch.float32))\n",
    "                test_losses.append(loss.item())\n",
    "        mean_test_loss = np.mean(test_losses)\n",
    "        if mean_test_loss < best_test:\n",
    "            best_val = mean_test_loss\n",
    "            early_stopping_logger[\"best_epoch\"] = epoch\n",
    "            early_stopping_logger[\"val_acc\"] = mean_test_loss\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                dir + \"/model_{}_{}.pth\".format(prog_args.dataset, epoch),\n",
    "            )\n",
    "        else:\n",
    "            print(\"Test loss is greater than the best val loss. Quitting...\")\n",
    "            return losses\n",
    "        torch.cuda.empty_cache()\n",
    "    return losses, early_stopping_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, early_stopping_logger = train(train_batched, model, get_default_args(), same_feat=True, val_dataset=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = 'model_52_4_PL.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "220-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
