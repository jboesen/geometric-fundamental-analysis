{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import curvlearn as cv\n","from curvlearn.manifolds.manifold import Manifold\n","import torch\n","import torch.nn as nn\n","from sklearn.feature_selection import mutual_info_regression\n","from sklearn.preprocessing import MinMaxScaler\n","from diffpool_helpers.model.diffpool_continuous import TSDiffPool\n","import argparse\n","import time\n","import random\n","from functools import lru_cache\n","import networkx as nx\n","import yfinance\n","import numpy as np\n","import pandas as pd\n","import yfinance as yf\n","from datetime import timedelta\n","import datetime\n","import requests\n","from math import floor\n","import dgl\n","from eodhd import APIClient\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{},"source":["# Import X and Y"]},{"cell_type":"markdown","metadata":{},"source":["### X"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["dt = {}\n","all_cols = None\n","Y = {}\n","for file in os.listdir('filtered_dt_428'):\n","    if file.endswith('.csv'):\n","        k = file.split('_')[0]\n","        dt[k] = pd.read_csv(os.path.join('filtered_dt_428', file))\n","        dt[k] = dt[k].loc[:, ~dt[k].columns.duplicated()]\n","        dt[k] = dt[k].reindex(sorted(dt[k].columns), axis=1)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import json\n","\n","with open('Y.json', 'r') as file:\n","    Y = json.load(file)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["for k, df in dt.items():\n","    # see if duplicate columns\n","    duplicate_columns = df.columns[df.columns.duplicated()]\n","    if len(duplicate_columns) > 0:\n","        print(f\"Duplicate columns found in DataFrame '{k}': {', '.join(duplicate_columns)}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def parse_columns(df):\n","    \"\"\"\n","    Function to parse column names and remove the {ticker}: prefix.\n","    \"\"\"\n","    new_columns = []\n","    for col in df.columns:\n","        if \":\" in col:\n","            ticker, real_col_name = col.split(\":\")\n","            new_columns.append(real_col_name)\n","        else:\n","            new_columns.append(col)\n","    return new_columns\n","\n","# Get the union of all column names across the DataFrames\n","all_cols = set().union(*[set(parse_columns(df)) for df in dt.values()])\n","list(all_cols)\n","\n","# Update the column names for each DataFrame\n","for k in list(dt.keys()):\n","    dt[k].columns = parse_columns(dt[k])\n","    try:\n","        dt[k] = dt[k].reindex(columns=all_cols, fill_value=0)\n","    except Exception as e:\n","        print(k, e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.unique([len(df.columns) for df in dt.values()])"]},{"cell_type":"markdown","metadata":{},"source":["## Y"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["with open('Y.json', 'r') as file:\n","    # Load the JSON data into a Python dictionary\n","    Y = json.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["# Autoencoder"]},{"cell_type":"markdown","metadata":{},"source":["# Drop range cols"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["range_list = pd.Series(range(max(len(df) for df in dt.values())))\n","\n","for k, df in dt.items():\n","    dt[k] = dt[k].drop(columns=[col for col in dt[k].columns if isinstance(col, range)])\n","    dt[k] = dt[k].drop(columns=[col for col in dt[k].columns if (dt[k][col] == range_list[:len(dt[k])]).all()])\n","    # Compute the mean and standard deviation for each column\n","    numeric_cols = df.select_dtypes(include=[np.number]).columns\n","\n","    # Compute the mean and standard deviation for each numerical column\n","    column_means = df[numeric_cols].mean(axis=0)\n","    column_stds = df[numeric_cols].std(axis=0) \n","    dt[k][numeric_cols] = (df[numeric_cols] - column_means) / column_stds"]},{"cell_type":"markdown","metadata":{},"source":["# Create Matrices"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# nodes\n","padded_tensors = []\n","max_length = 7\n","for df in dt.values():\n","    num_cols = df.columns[df.dtypes != 'object']\n","    df = df[num_cols]\n","    # get first 7 rows\n","    padded_tensor = torch.tensor(df.iloc[-7:].values.tolist())\n","    padded_tensors.append(padded_tensor)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# T x batch size x features\n","X = torch.stack(padded_tensors, dim=1)\n","\n","mean_tensor = torch.mean(X, dim=(0, 1), keepdim=True)\n","std_tensor = torch.std(X, dim=(0, 1), unbiased=False, keepdim=True)\n","X = (X - mean_tensor) / std_tensor\n","\n","padded_tensors = []\n","max_length = max(len(df) for df in dt.values())"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["ncol = np.unique([len(v.columns) for v in dt.values()])[0]\n","adj = torch.zeros(ncol, ncol, dtype=torch.float32)\n","for i in range(ncol):\n","    for j in range(ncol):\n","        adj[i, j] = torch.tensor(mutual_info_regression(X[:, i].reshape(-1, 1) , X[:, j].reshape(-1, 1).ravel()))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["with open('X.pkl', 'wb') as f:\n","    pickle.dump(X, f)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["adj = adj / adj.std()"]},{"cell_type":"markdown","metadata":{},"source":["## Create Graphs"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from sklearn.feature_selection import mutual_info_regression\n","max_t, num_symbols, num_features = X.shape\n","\n","# Create a tensor to store the temporal edge weights\n","temporal_edge_weights = torch.zeros((num_features, num_features, max_t - 1))\n","\n","for t in range(1, max_t):\n","    for i in range(num_features):\n","            # Calculate the absolute difference between the node's values at time t and t-1\n","            temporal_edge_weights[i, i, t-1] = torch.tensor(mutual_info_regression(X[t, :, i].unsqueeze(-1), X[t-1, :, i].unsqueeze(-1).ravel()))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def create_graph(symbol_idx, k, dt=dt):\n","    \"\"\"\n","    Create a graph representation from the given data tensor and adjacency matrix.\n","\n","    Args:\n","       t (int): Time step index.\n","       symbol_idx (int): Index of the symbol (e.g., stock index).\n","       k (str): Key for accessing the data tensor in the dt dictionary.\n","       dt (dict, optional): Dictionary containing data tensors. Defaults to the global dt variable.\n","\n","    Returns:\n","       dgl.DGLGraph: A DGLGraph object representing the input data and adjacency matrix.\n","    \"\"\"\n","    # only take the last seven rows\n","    df = dt[k]\n","    weights = []\n","    n1 = []\n","    n2 = []\n","    num_cols = df[df.columns[df.dtypes != 'object']]\n","    n_feats = len(num_cols.columns)\n","\n","    node_data = [None for _ in range(7 * n_feats)]\n","    node_data = torch.zeros((7 * n_feats, 2), dtype=torch.float64)\n","    for t in range(6):\n","        for i, col1 in enumerate(num_cols):\n","            for j, col2 in enumerate(num_cols):\n","                if col1 != col2 and adj[i, j] > -1:\n","                    n1.append(t * n_feats + i)\n","                    n2.append(t * n_feats + j)\n","                    weights.append(adj[i, j])\n","\n","            node_data[t * n_feats + j, ] = torch.tensor([X[t, symbol_idx, i], t], dtype=torch.float64)\n","            n1.append(t * n_feats + i)\n","            n2.append((t + 1) * n_feats + i)\n","            weights.append(temporal_edge_weights[i, i, t])\n","\n","    G = dgl.graph((torch.tensor(n1), torch.tensor(n2)))\n","    # set node data\n","    G.ndata['feat'] = node_data\n","    # set edge data\n","    G.edata['x'] = torch.tensor(weights, dtype=torch.float64)\n","    return G"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["max_t = X.shape[0]\n","graphs = []\n","batched_graphs = []\n","\n","# Assuming dt is an iterable (e.g., list, numpy array, etc.)\n","for i, k in enumerate(dt):\n","    if k in Y:\n","        # Create a graph from the data tensor and append it to the list of graphs\n","        batched_graphs.append((create_graph(i, k), Y[k]))\n","    # Batch the graphs and append them along with the corresponding label Y[i]\n","    \n","\n","# Create a PyTorch DataLoader from the batched_graphs\n","train_data = batched_graphs\n","# train_data = torch.utils.data.DataLoader(batched_graphs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batched_graphs[0][0].ndata['feat'].shape"]},{"cell_type":"markdown","metadata":{},"source":["# Train/test split"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["test_size = int(0.1 * len(batched_graphs))\n","test_set = batched_graphs[:test_size]\n","train_set = batched_graphs[test_size:]"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["import pickle\n","\n","# Save test set\n","with open('test_set.pkl', 'wb') as f:\n","    pickle.dump(test_set, f)\n","\n","# Save train set\n","with open('train_set.pkl', 'wb') as f:\n","    pickle.dump(train_set, f)"]}],"metadata":{"kernelspec":{"display_name":"220-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
